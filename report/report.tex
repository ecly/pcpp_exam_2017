\documentclass[a5paper]{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[final]{pdfpages}
\usepackage[parfill]{parskip}% don't indent new sections
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage[a4paper]{geometry}
\pagestyle{empty}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Java,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\title{Practical Concurrent and Parallel Programming}
\author{Emil Lynegaard}

\begin{document}
\includepdf[pages=1]{res/front_page.pdf}
\maketitle
\textit{I hereby declare that I have answered thede exam questions myself without any outside help.}\\

Through all tests, the same machine will be used. Below are the results of \texttt{SystemInfo}:


\begin{table}[!ht]
\begin{center}
\begin{tabular}{ l l }
OS & Linux; 4.13.12-1-ARCH; amd64\\
JVM & Oracle Corporation; 1.8.0\_144\\
CPU & null; 8 "cores"\\
Date & 2017-12-11T09:20:13+0100
\end{tabular}
\end{center}
\caption{System Info}
\label{sysinfo}
\end{table}

\section{Question 1}
\subsection{}
Seeing as we are interested in seeing how well each implementation performs on random input of different sizes, we use Mark9 for the benchmarking, as it calculates the per element mean time and standard deviation.

\begin{figure}[!ht]
    \centering
    \noindent\includegraphics[scale=0.5]{res/graph_q1.png}
    \caption{Plot of running times of given implementations. Tested with size = 10000000.}
    \label{fig:graphq1}
\end{figure}

From graph \ref{fig:graphq1} we see that, perhaps unsurprisingly, the serial quickselect and quickcount implementations beat out serial sort entirely.
We do however see parallel sort get rather close to the expected linear running time implementations, due to the test machine having a quad-core CPU,
allowing an approximate 4 time speedup from the serial implementation. Between the expected linear running time algorithms, it seems serial quickselect 
beat out its quickcount counterparts, however the difference is so small that we have yet to find a definitive winner.

\subsection{}


\section{Question 2}
\subsection{}
\begin{lstlisting}
      (int) Arrays.stream(inp).skip(1).filter(i -> i < p).count();
\end{lstlisting}
Where \texttt{inp} is our input array, and \texttt{p} is our current partition candidate.
Skip the first element as this is the index of the partition element with which we do not wish to compare.

\subsection{}
\begin{lstlisting}
          Arrays.stream(inp).skip(1).filter(i -> i < p).toArray();
          Arrays.stream(inp).skip(1).filter(i -> i >= p).toArray();
\end{lstlisting}

\subsection{}
\begin{lstlisting}
public static int quickCountStream(int[] inp) {
    int partition=-1, count=0, n=inp.length;
    int target = n/2;
    do {
        partition=inp[0];
        final int p = partition;
        n=inp.length;
        count = (int) Arrays.stream(inp).skip(1).filter(i -> i < p).count();
        if (count == target) break;
        if (count > target){
            inp = Arrays.stream(inp).skip(1).parallel.()filter(i -> i < p).toArray();
        }else{
            inp = Arrays.stream(inp).skip(1).parallel().filter(i -> i >= p).toArray();
            target=target-count-1;
        }
    } while( true );
    return partition; // we are on target
}
\end{lstlisting}

Combining the two we get above implementation, which yields correct results.

\subsection{}
\begin{lstlisting}
    Arrays.stream(inp).parallel().skip(1).filter(i -> i < p).count();
    Arrays.stream(inp).parallel().skip(1).filter(i -> i < p).toArray();
    Arrays.stream(inp).parallel().skip(1).filter(i -> i >= p).toArray();
\end{lstlisting}
Here we simply throw \texttt{.parallel()} onto the pipelines from before.

\subsection{}

\begin{lstlisting}
public static int quickCountStream(int[] inp) {
    int partition=-1;
    int target = inp.length/2;
    // Since we have to be working with boxed Integers. We start off by converting.
    List<Integer> list = Arrays.stream(inp).boxed().collect(Collectors.toList());
    do {
        partition = list.get(0);
        final Integer p = partition;
        Map<Boolean, List<Integer>> res = list.stream().skip(1).parallel()
            .collect(Collectors.partitioningBy(i -> i < p));

        List<Integer> smaller = res.get(true);
        System.out.println(Arrays.toString(smaller.toArray()));
        List<Integer> bigger = res.get(false);
        System.out.println(Arrays.toString(bigger.toArray()));

        if (smaller.size() == target) break;
        if (smaller.size() > target) list = smaller;
        else {
            target=target-smaller.size()-1;
            list = bigger;
        }
   } while( true );
    return partition; // we are on target
}
\end{lstlisting}
To avoid having to constantly do boxing, since Collectors does not work with primitives,
we start off by converting our \texttt([] inp) to \texttt{List<Integer>}.

The \texttt{partitioningBy} collector gives us a map of all with two entries.
The \texttt{true} entry on line 12, holding all elements larger than our partition element, and false entry on line 14 holding
all the elements larger than or equal to our partition element. The size of \texttt{smaller} now represents our \texttt{count}
from before, and the remainder of the code is similar to the given code.

\subsection{TODO} 

\section{Question 3 - TODO}
By initially deciding what ranges threads will work on for the entire algorithm, we risk a thread winding up
with a range of the input that we can discard as useless after a single iteration. 

\section{Question 4}
\subsection{}
We are not guaranteed to always grab locks in the same order, hence we are prone to dead-locks.

\subsection{}

\begin{lstlisting}
union(0,1)
union(1,0)
\end{lstlisting}

Above example when executed in parallel may lead to the first call grabbing the lock on \texttt{nodes[0]}, the second call grabbing the lock on \texttt{nodes[1]} and both calls thereafter waiting to get the lock on the node that their counterpart already grabbed.

\subsection{}
We modify the given \texttt{concurrent()} method in class \texttt{UnionFindTest} in file \texttt{MyUnionFind.java} to target the issue we identified in 4.1.
\begin{lstlisting}
public void deadlock(final int size, final UnionFind uf) throws Exception {
    final int[] numbers = new int[size];
    for (int i = 0; i < numbers.length; ++i) numbers[i] = i;
    final int threadCount = 32;
    final CyclicBarrier startBarrier = new CyclicBarrier(threadCount+1), 
          stopBarrier = startBarrier;
    Collections.shuffle(Arrays.asList(numbers));
    for (int i = 0; i < threadCount; ++i) {
        final boolean reverse = i%2==0;
        Thread ti = new Thread(new Runnable() { public void run() {
            try { startBarrier.await(); } catch (Exception exn) { }
            if (reverse)
                for (int j=0; j<100; j++)
                    for (int i = 0; i < numbers.length - 1; ++i) 
                        uf.union(numbers[i], numbers[i + 1]);
            else 
                for (int j=0; j<100; j++)
                    for (int i = 0; i < numbers.length - 1; ++i) 
                        uf.union(numbers[i + 1], numbers[i]);
            try { stopBarrier.await(); } catch (Exception exn) { }
        }});
        ti.start();
    }
    startBarrier.await();
    stopBarrier.await();
    final int root = uf.find(0);
    for (int i : numbers) {
        assertEquals(uf.find(i), root);
    }
    System.out.println("No deadlocks");
}
\end{lstlisting}

As seen from line 10 to 20, half the threads will now be attempting to union nodes in reverse order of the other half.
From my tests, calling the above defined \texttt{deadlock} method with parameters shown below, deadlocked every time.

\begin{lstlisting}
UnionFindTest test = new UnionFindTest();
test.deadlock(itemCount, new BogusFineUnionFind(itemCount));
\end{lstlisting}

By merely adding a check to the given \texttt{union()} method in class \texttt{BogusFineUnionFind} to ensure we always lock the lowest entry of the \texttt{nodes} array first, the deadlock test
executes without deadlocking.

\section{Question 5}
\textbf{Specifications}:
\begin{enumerate}
\item pop returns an inserted item or the value null. It might block until another concurrent operation completes,
but it will return without delay if no other operation is happening simultaneously. In particular, it will not
block until another thread inserts some element.
\item for each element that is pushed, there is at most one pop operation that returns that element.
\item If there are no further concurrent operations, pop will succeed (i.e. return a non-null value) if so far there
have been more successful push than pop operations.
\item If processor A pushed two elements x and y in this order, and processor B pops both elements, then this
happens in reverse order. (There is no further constraint on ordering).
\end{enumerate}

\subsection{}\label{sec:mystacksimple}
\begin{lstlisting}
import java.util.LinkedList;
public class MyStack<T> {
    private Object lock;
    private LinkedList<T> stack;

    public MyStack(){
        lock = new Object();
        stack = new LinkedList<T>();
    }

    public void push(T obj) {
        synchronized(lock){
            stack.push(obj);
        }
    }

    public T pop() {
        synchronized(lock){
            return stack.peek() != null ? stack.pop() : null;
        }
    }
}
\end{lstlisting}

Above generic implementation utilizes that Java's \texttt{LinkedList} ships with \texttt{push} and \texttt{pop}.
Alternatively we could use the combination \texttt{addLast} and \texttt{removeLast} or \texttt{addFirst} and \texttt{removeFirst} 
of which the second pair by Java's documentation is equivalent to \texttt{push} and \texttt{pop}\footnote{https://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html}. 
In \texttt{pop}, given that the specification states that we should return null if the list is empty, we use \texttt{peek} to check if there is a first element, if there isn't return null, otherwise \texttt{pop}.
The locking could also be done implicitly on \texttt{this} by marking the methods as synchronized, but here we use an explicit lock object as it seems more in line with specification number 1.

\subsection{}
Given the initially stated  specifications, below are added bullet points (letters) matching specifications, describing why the implementation from \ref{sec:mystacksimple} is sufficient.
\begin{enumerate}

\item 
    \begin{enumerate}
        \item We take care of this explicitly in the implementation for \texttt{MyStack} in section \ref{sec:mystacksimple}, where we on line 19, \texttt{peek} prior to popping.
            If we blindly popped on an empty list, we would get a \texttt{NoSuchElementException}\footnote{https://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html\#pop()}.
    \end{enumerate}
\item
    \begin{enumerate}
        \item Given that \texttt{pop} removes the single first element, and \texttt{push} only adds the element once, \texttt{pop} may only return an element pushed once.
        \item Furthermore, since we are using a single lock, everything happens sequentially, removing any chance of reading off an element that was removed in a different thread.
    \end{enumerate}
\item
    \begin{enumerate}
        \item Same as 2.a.
    \end{enumerate}
\item 
    \begin{enumerate}
        \item Since everything is handled sequentially due to the global lock, we have the guarantee that if one thread pushes two items, these will be pushed
            in the order of which the thread called push. 
        \item Given 3.a. and Java's Documentation stating that \texttt{push} and \texttt{pop} adds/removes from the head of the list, elements are bound to be popped
            in reverse compared to the order of which they were pushed.
    \end{enumerate}
\end{enumerate}

\subsection{}
\subsubsection{Concurrency}
To test for everything except reverse ordering, we define the below shown test \texttt{concurrentTest}.
\begin{lstlisting}
public static void concurrentTest(final int size, final int threads,  
        MyStack<Integer> stack) throws Exception {
    final CyclicBarrier startBarrier = new CyclicBarrier(threads+1), 
          stopBarrier = startBarrier;

    final int range = size/threads;
    for (int i = 0; i < threads; ++i) {
        final int nr = i;
        Thread ti = new Thread(new Runnable() { public void run() {
            try { startBarrier.await(); } catch (Exception exn) { }
                for(int j = range*nr; j<range*nr+range; j++)
                    stack.push(j);
            try { stopBarrier.await(); } catch (Exception exn) { }
        }});
        ti.start();
    }
    startBarrier.await();
    stopBarrier.await();
    startBarrier.reset();
    stopBarrier.reset();

    final Set<Integer> pops = ConcurrentHashMap.newKeySet();
    for (int i = 0; i < threads; ++i) {
        final int nr = i;
        Thread ti = new Thread(new Runnable() { public void run() {
            try { startBarrier.await(); } catch (Exception exn) { }
                for(int j = range*nr; j<range*nr+range; j++)
                    pops.add(stack.pop());
            try { stopBarrier.await(); } catch (Exception exn) { }
        }});
        ti.start();
    }

    startBarrier.await();
    stopBarrier.await();

    if (pops.size() == size) System.out.println("Concurrency works :)");
    else  System.out.println("Concurrency doesn't work :(");
}
\end{lstlisting}

We create \texttt{CyclicBarrier}s on line 3, which we use to ensure that our threads are working simultaenously. 
We then assign different threads ranges between 0 and \texttt{size} in which they will push all numbers to the stack.

When all the threads are ready, we call \texttt{startBarrier.await()} on line 17 to start the pushing and \texttt{stopBarrier.await()} on the next line
to wait for all of them to finish pushing. 

We now divide the work similarly for popping threads. For each number they pop, we add it to the \texttt{ConcurrentHashMap} backed \texttt{Set} defined on line 22. 
When all threads are done popping, we can check whether the size of our \texttt{Set} is equal to the number of items we tried to add. 
Since there are no duplicates in a \texttt{Set}, and we only pushed unique numbers, if these are equal we can with reasonably confidence say that our implementation is working.

\textbf{Scaling:} In terms of scaling, since MyStack functions entirely sequentially, it scales poorly with multiple threads. In fact, the more threads we use, the more time will be spent 
locking and waiting for locks, making it slower and slower. In table \ref{table:concTest} behavior is illustrated.

\begin{table}[ht!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Threads & 1 & 2 & 4 & 8 & 16 & 32 \\ \hline
Time (sec) & 7.363 & 8.159 & 8.203 & 8.413 & 10.071 & 10.832 \\ \hline
\end{tabular}
\caption{Test times for conccurentTest with size 10,000,000}
\label{table:concTest}
\end{table}
\subsubsection{Ordering} \label{sec:ordering}
To test that order works for multiple threads, we push number $[0..n]$ onto the stack from thread A, and pop \texttt{n} items off the stack from thread B,
checking that these are the numbers $[n..0]$.
This is shown below in method \texttt{testOrder}.

\begin{lstlisting}
public static void testOrder(int n, MyStack<Integer> stack){
    final AtomicBoolean working = new AtomicBoolean(true);
    Thread A = new Thread(new Runnable() { public void run() {
        for(int i=0; i<n; i++) stack.push(i);
    }});
    Thread B = new Thread(new Runnable() { public void run() {
        for(int i=0; i<n; i++) 
            working.compareAndSet(true, n-1-i == stack.pop());
    }});
    try {
        A.start(); A.join();
        B.join(); B.join();
    } catch (Exception e) { 
        System.out.println("Order dies >:("); 
    }
    if(working.get()) System.out.println("Order works :)");
    else System.out.println("Order doesn't work :(");
}
\end{lstlisting}

This behaviour was untested in \texttt{concurrentTest}, so a separate straight forward test to clear this up was needed.

\subsection{}\label{sec:striping}
Below is shown an implentation using striping, with a total of 32 stripes. To determine the stripe use \texttt{Thread.currentThread()hashCode()\%STRIPES)}
as shown on line 16 and 24. We use an \texttt{ArrayList} to store our \texttt{LinkedList}s since we cannot create arrays of 
parameterized types\footnote{https://docs.oracle.com/javase/tutorial/java/generics/restrictions.html\#createArrays}.

In \texttt{pop}, we use \texttt{i\%STRIPES} to iterate through the stacks, starting from the stack at the computed \texttt{stripe}, with wraparound.

\begin{lstlisting}
import java.lang.*;
import java.util.*;
public class MyStack<T> {
    private Object lock;
    private final List<LinkedList<T>> stacks;
    private static final int STRIPES = 32;

    public MyStack(){
        lock = new Object();
        stacks = new ArrayList<LinkedList<T>>();
        for(int i = 0; i < STRIPES; i++)
            stacks.add(new LinkedList<T>());
    }

    public void push(T obj) {
        int stripe = Thread.currentThread().hashCode()%STRIPES;
        LinkedList<T> stack = stacks.get(stripe);
        synchronized(stack){
            stack.push(obj);
        }
    }

    public T pop() {
        int stripe = Thread.currentThread().hashCode()%STRIPES;
        for (int i = stripe; i < stripe+STRIPES; i++){
            LinkedList<T> stack = stacks.get(i%STRIPES);
            synchronized(stack){
                if(stack.size() == 0) continue;
                return stack.pop();
            }
        }
        return null;
    }
}
\end{lstlisting}

\subsection{}
With the new striping such that we are actually running concurrently we see improved performance in our tests.
The fastest execution, as seen in table \ref{table:striping} was with 16 threads where it ran in 5.866 seconds. This is in contrast
to the single threaded execution from table \ref{table:concTest} that ran in 7.363 seconds with a single thread. Considering that we have
4 physical cores available, this is not that big of a performance improvement. This could be due to many threads hashing to the same stripe or
due to the added iteration through all the stacks when we run into an empty one.


\begin{table}[ht!]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Threads & 1 & 2 & 4 & 8 & 16 & 32 \\ \hline
Time (sec) & 8.016 & 7.473 & 7.465 & 6.313 & 5.866 & 7.337 \\ \hline
\end{tabular}
\caption{Test times for MyStack with striping for conccurentTest with size 10,000,000}
\label{table:striping}
\end{table}

\subsection{}
Given our implementation described in \ref{sec:striping}, we modify line 29 to say:
\begin{lstlisting}
return i == stripe ? stack.pop() : stack.removeLast();
\end{lstlisting}
If we are on our own stripe's stack, pop from the front, otherwise pop from the back.

\subsection{}
Below is a description in pseudocode of what will go wrong given our changes in 5.6.

\begin{lstlisting}
//Thread A // Stripe 0
myStack.push(0);
myStack.push(1);

//Thread B // Stripe 1
myStack.pop(); // this will return 0 - should be 1
myStack.pop(); // this will return 1 - should be 0
\end{lstlisting}

\subsection{}
The code described in section \ref{sec:ordering} detects this issue and outputs "Order doesn't work :(" as expected.

\section{Question 6}
\subsection{}
The given code has the flaw that it allows multiple threads to get into the else block before anyone changes the \texttt{state}.
This means that in an example with thread \texttt{A} calling \texttt{consensus(x)} and thread \texttt{B} calling \texttt{consensus(y)}, both may retrieve return values
indicating that parameter value \texttt{x} or \texttt{y} is the consensus, whereas only one of them will be stored in \texttt{state}. Hence we have a race condition.

\subsection{}
Using synchronization on some lock, in this case on \texttt{this}, has a problem with termination. If one process is to fail while holding the lock, or fall asleep for an extended amount of time,
the whole system will halt as they wait to retrieve the lock. As such, this implementation is not fault tolerant.

\subsection{}
Firstly, assuming that we're talking Java, this would fail to typecheck, as we're returning an \texttt{AtomicInteger} from a method that supposedly returns an \texttt{int} and there is no implicit conversion from \texttt{AtomicInteger} to \texttt{int}. Even with this fixed, it would still fail, since two threads may enter the while loop before state changes, causing one of them to be stuck in the loop forever, since \texttt{state.compareAndSet(-1,x)} will then be returning false over and over. This will work if only one thread ever gets to enter the while loop and successfully \texttt{compareAndSet}s the \texttt{state}, since all other threads then merely get the value of the state, leading to consensus.

\section{Question 7}
\end{document}
